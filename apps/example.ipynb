{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PINN for exponential decay with parameter identification\n",
    "ODE: dy/dt + k y = 0,  y(0)=y0\n",
    "Learn both the function y(t) and the decay constant k from sparse (possibly noisy) data.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        net = []\n",
    "        for i in range(len(layers) - 2):\n",
    "            net.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            net.append(nn.Tanh())\n",
    "        net.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (N, 1)\n",
    "        return self.net(t)\n",
    "\n",
    "\n",
    "def make_synthetic_data(k_true=1.7, y0=2.0, t_max=2.0, n_data=20, noise_std=0.02, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    t_data = rng.uniform(0.0, t_max, size=(n_data, 1))\n",
    "    t_data = np.sort(t_data, axis=0)\n",
    "\n",
    "    y_clean = y0 * np.exp(-k_true * t_data)\n",
    "    y_noisy = y_clean + rng.normal(0.0, noise_std, size=y_clean.shape)\n",
    "\n",
    "    return t_data, y_noisy, y_clean\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # True system\n",
    "    k_true = 1.7\n",
    "    y0 = 2.0\n",
    "    t_max = 2.0\n",
    "\n",
    "    # Data (sparse, noisy)\n",
    "    t_data_np, y_data_np, _ = make_synthetic_data(\n",
    "        k_true=k_true, y0=y0, t_max=t_max, n_data=20, noise_std=0.02, seed=0\n",
    "    )\n",
    "\n",
    "    # Collocation points for physics loss\n",
    "    n_f = 200\n",
    "    t_f_np = np.linspace(0.0, t_max, n_f).reshape(-1, 1)\n",
    "\n",
    "    # Torch tensors\n",
    "    device = torch.device(\"cpu\")\n",
    "    t_data = torch.tensor(t_data_np, dtype=torch.float32, device=device)\n",
    "    y_data = torch.tensor(y_data_np, dtype=torch.float32, device=device)\n",
    "    t_f = torch.tensor(t_f_np, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "    t0 = torch.tensor([[0.0]], dtype=torch.float32, device=device)\n",
    "    y0_t = torch.tensor([[y0]], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Network + trainable parameter\n",
    "    model = MLP(layers=[1, 10, 1]).to(device)\n",
    "\n",
    "    # Enforce k>0 via softplus\n",
    "    raw_k = nn.Parameter(torch.tensor([0.0], dtype=torch.float32, device=device))  # initial guess\n",
    "    params = list(model.parameters()) + [raw_k]\n",
    "\n",
    "    # Loss weights (you can tune these)\n",
    "    alpha = 1.0   # data\n",
    "    beta = 1.0    # equation\n",
    "    gamma = 10.0  # initial condition\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "    # Training\n",
    "    epochs = 8000\n",
    "    k_hist = []\n",
    "    loss_hist = []\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k_hat = F.softplus(raw_k)  # positive decay constant\n",
    "\n",
    "        # ---------\n",
    "        # Physics loss: Leq = mean( (dyhat/dt + k_hat*yhat)^2 )\n",
    "        # ---------\n",
    "        y_f = model(t_f)  # (Nf,1)\n",
    "        dy_dt = torch.autograd.grad(\n",
    "            outputs=y_f,\n",
    "            inputs=t_f,\n",
    "            grad_outputs=torch.ones_like(y_f),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        residual = dy_dt + k_hat * y_f\n",
    "        L_eq = torch.mean(residual**2)\n",
    "\n",
    "        # ---------\n",
    "        # Initial condition loss: Lic = (yhat(0)-y0)^2\n",
    "        # ---------\n",
    "        y_0_pred = model(t0)\n",
    "        L_ic = torch.mean((y_0_pred - y0_t) ** 2)\n",
    "\n",
    "        # ---------\n",
    "        # Data loss: Ldata = mean( (yhat(t_i)-y_i)^2 )\n",
    "        # ---------\n",
    "        y_d_pred = model(t_data)\n",
    "        L_data = torch.mean((y_d_pred - y_data) ** 2)\n",
    "\n",
    "        # Total\n",
    "        loss = alpha * L_data + beta * L_eq + gamma * L_ic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track\n",
    "        k_hist.append(float(k_hat.detach().cpu().numpy().squeeze()))\n",
    "        loss_hist.append(float(loss.detach().cpu().numpy()))\n",
    "\n",
    "        if ep % 1000 == 0 or ep == 1:\n",
    "            print(\n",
    "                f\"epoch={ep:5d}  loss={loss_hist[-1]:.3e}  \"\n",
    "                f\"L_data={float(L_data.detach().cpu()):.3e}  \"\n",
    "                f\"L_eq={float(L_eq.detach().cpu()):.3e}  \"\n",
    "                f\"L_ic={float(L_ic.detach().cpu()):.3e}  \"\n",
    "                f\"k_hat={k_hist[-1]:.6f}\"\n",
    "            )\n",
    "\n",
    "    # Evaluation on a dense grid\n",
    "    t_test_np = np.linspace(0.0, t_max, 300).reshape(-1, 1)\n",
    "    t_test = torch.tensor(t_test_np, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(t_test).cpu().numpy().squeeze()\n",
    "\n",
    "    y_true = (y0 * np.exp(-k_true * t_test_np)).squeeze()\n",
    "\n",
    "    print(\"\\nFinal results:\")\n",
    "    print(f\"  true k  = {k_true:.6f}\")\n",
    "    print(f\"  learned k_hat = {k_hist[-1]:.6f}\")\n",
    "\n",
    "    # Print a few predictions\n",
    "    sample_times = np.array([0.0, 0.5, 1.0, 1.5, 2.0]).reshape(-1, 1)\n",
    "    sample_t = torch.tensor(sample_times, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        sample_pred = model(sample_t).cpu().numpy().squeeze()\n",
    "    sample_true = (y0 * np.exp(-k_true * sample_times)).squeeze()\n",
    "\n",
    "    print(\"\\nPredictions at selected times:\")\n",
    "    for tt, yt, yp in zip(sample_times.squeeze(), sample_true, sample_pred):\n",
    "        print(f\"  t={tt:>4.1f}  y_true={yt:.6f}  y_pred={yp:.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plots: y(t) and k learning\n",
    "    # ----------------------------\n",
    "    plt.figure(dpi=300)\n",
    "    plt.plot(t_test_np, y_true, label=\"Simulation\")\n",
    "    plt.plot(t_test_np, y_pred, label=\"PINN\")\n",
    "    plt.scatter(t_data_np, y_data_np.squeeze(), label=\"Data\")\n",
    "    plt.xlabel(r\"\\(t\\)\")\n",
    "    plt.ylabel(r\"\\(y(t)\\)\")\n",
    "    plt.legend()\n",
    "    # plt.title(\"Exponential decay: PINN prediction\")\n",
    "\n",
    "    plt.figure(dpi=300)\n",
    "    plt.plot(np.arange(1, epochs + 1), k_hist, label=\"k_hat\")\n",
    "    plt.axhline(k_true, linestyle=\"--\", label=\"true k\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(r\"Decay Constant \\(k\\)\")\n",
    "    plt.legend()\n",
    "    # plt.title(\"Learning the decay constant\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
